{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b38f8eb",
   "metadata": {},
   "source": [
    "# BLIPâ€‘2 Demo â€” Captioning and VQA (Transformers)\n",
    "\n",
    "This notebook demonstrates image captioning and visual question answering with **BLIPâ€‘2 (Flanâ€‘T5 XL)** using ðŸ¤— Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f449376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab or a fresh environment, uncomment the next lines:\n",
    "# !pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -U transformers accelerate pillow safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'Salesforce/blip2-flan-t5-xl'\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(MODEL_ID)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.float16 if device == 'cuda' else torch.float32, device_map=None\n",
    ").to(device)\n",
    "\n",
    "print('Loaded model:', MODEL_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de485fa",
   "metadata": {},
   "source": [
    "## Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your image path or URL here\n",
    "IMAGE_PATH = 'sample.jpg'  # e.g., '/path/to/your_image.jpg'\n",
    "\n",
    "# Load image\n",
    "image = Image.open(IMAGE_PATH).convert('RGB')\n",
    "\n",
    "# Generate caption\n",
    "inputs = processor(images=image, return_tensors='pt').to(device, dtype=torch.float16 if device=='cuda' else torch.float32)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print('Caption:', caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee32f5",
   "metadata": {},
   "source": [
    "## Visual Question Answering (Zeroâ€‘shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385946fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about the same image\n",
    "question = 'What is the main object in the image?'\n",
    "\n",
    "prompt = f'Question: {question} Short answer:'  # per BLIP-2 Flanâ€‘T5 prompt style\n",
    "inputs = processor(images=image, text=prompt, return_tensors='pt').to(device, dtype=torch.float16 if device=='cuda' else torch.float32)\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
    "answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print('Q:', question)\n",
    "print('A:', answer)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
